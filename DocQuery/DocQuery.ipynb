{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0JpbN15WuQW"
      },
      "source": [
        "# The Apprentice's Quest - The Personal Document Oracle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz0Q9Al-WVy0",
        "outputId": "8795696e-3e5f-474f-eca9-f20f74e2784a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Your Groq API Key:··········\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Import libraries and set up API Keys\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter Your Groq API Key:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmeByNQnW5HX",
        "outputId": "3c7458b6-a350-45f9-acde-7148a7de2eab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-06-07 17:15:05--  https://arxiv.org/pdf/1706.03762.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.3.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/1706.03762 [following]\n",
            "--2025-06-07 17:15:05--  http://arxiv.org/pdf/1706.03762\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2215244 (2.1M) [application/pdf]\n",
            "Saving to: ‘attention_is_all_you_need.pdf’\n",
            "\n",
            "\r          attention   0%[                    ]       0  --.-KB/s               \rattention_is_all_yo 100%[===================>]   2.11M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-06-07 17:15:05 (40.8 MB/s) - ‘attention_is_all_you_need.pdf’ saved [2215244/2215244]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Download the sample document (if needed)\n",
        "!wget -O attention_is_all_you_need.pdf https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj6mwnOFW5Q_",
        "outputId": "1d603a94-3fc3-4130-d758-0f6067b47b54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the document...\n",
            "Splitting the document into chunks...\n",
            "Downloading and creating Hugging Face embeddings...\n",
            "Embeddings model loaded.\n",
            "Vector store created.\n",
            "Creating the QA chain with Groq...\n",
            "Fully free RAG pipeline is ready!\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: The Core RAG Logic (Hugging Face Edition)\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # <-- Import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# 1. Load the document\n",
        "print(\"Loading the document...\")\n",
        "loader = PyPDFLoader(\"attention_is_all_you_need.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "# 2. Split the document into chunks\n",
        "print(\"Splitting the document into chunks...\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "\n",
        "# 3. Create embeddings using Hugging Face and store in FAISS\n",
        "print(\"Downloading and creating Hugging Face embeddings...\")\n",
        "# Use a specific, well-performing model\n",
        "model_name = \"all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name) # <-- Use HuggingFaceEmbeddings\n",
        "print(\"Embeddings model loaded.\")\n",
        "\n",
        "db = FAISS.from_documents(texts, embeddings)\n",
        "print(\"Vector store created.\")\n",
        "\n",
        "# 4. Create the retriever\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# 5. Create the QA chain with Groq LLM\n",
        "print(\"Creating the QA chain with Groq...\")\n",
        "llm = ChatGroq(\n",
        "    model_name=\"llama3-8b-8192\",\n",
        "    temperature=0\n",
        ")\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "print(\"Fully free RAG pipeline is ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoGbvN8cW5dC",
        "outputId": "f0d1eb6d-3aea-4aec-df2c-425590238c07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Answer ---\n",
            "A Transformer is a type of neural network architecture primarily used for natural language processing (NLP) tasks, such as machine translation, text summarization, and language modeling. The main components of a Transformer are:\n",
            "\n",
            "1. Encoder: The encoder is responsible for processing the input sequence (e.g., a sentence or paragraph) and generating a continuous representation of the input.\n",
            "2. Decoder: The decoder generates the output sequence (e.g., the translated sentence) based on the input sequence and the encoder's output.\n",
            "3. Multi-Head Attention: This is a key component of the Transformer, allowing the model to attend to different parts of the input sequence simultaneously and weigh their importance.\n",
            "4. Self-Attention: The Transformer uses self-attention mechanisms in both the encoder and decoder, allowing each position in the sequence to attend to all other positions.\n",
            "5. Positional Encoding: The Transformer uses positional encoding to provide the model with information about the position of each token in the input sequence.\n",
            "6. Feed-Forward Network (FFN): The FFN is a fully connected feed-forward network used in the encoder and decoder to transform the output of the self-attention mechanism.\n",
            "\n",
            "The Transformer architecture is designed to handle sequential data and is particularly effective for tasks that require modeling long-range dependencies in the input sequence.\n",
            "\n",
            "--- Sources ---\n",
            "- Page 4: The Transformer uses multi-head attention in three different ways:\n",
            "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
            "and the memory keys and values come from the output of the encoder. This allows every\n",
            "positi...\n",
            "- Page 2: Figure 1: The Transformer - model architecture.\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
            "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
            "re...\n",
            "- Page 0: best models from the literature. We show that the Transformer generalizes well to\n",
            "other tasks by applying it successfully to English constituency parsing both with\n",
            "large and limited training data.\n",
            "∗Equal contribution. Listing order is random. Jakob p...\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Ask a question\n",
        "question = \"What is a transformer and what are its main components?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "\n",
        "print(\"\\n--- Answer ---\")\n",
        "print(result[\"result\"])\n",
        "\n",
        "print(\"\\n--- Sources ---\")\n",
        "for source in result[\"source_documents\"]:\n",
        "    print(f\"- Page {source.metadata['page']}: {source.page_content[:250]}...\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
